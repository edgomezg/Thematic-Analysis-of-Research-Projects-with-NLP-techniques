{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hMMWxeB-IP6"
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1703712469927,
     "user": {
      "displayName": "OIER HUICI ITARTE",
      "userId": "12479912714830698396"
     },
     "user_tz": -60
    },
    "id": "Ocvl_CTe-Dxu",
    "outputId": "0ba22dfe-3ac9-45e0-d9b2-aa36f26792bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "#Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def check_nltk_packages():\n",
    "  packages = ['punkt','stopwords','omw-1.4','wordnet']\n",
    "\n",
    "  for package in packages:\n",
    "    try:\n",
    "      nltk.data.find('tokenizers/' + package)\n",
    "    except LookupError:\n",
    "      nltk.download(package)\n",
    "check_nltk_packages()\n",
    "\n",
    "try:\n",
    "  import lxml\n",
    "except ModuleNotFoundError:\n",
    "  %pip install lxml\n",
    "\n",
    "try:\n",
    "  import contractions\n",
    "except ModuleNotFoundError:\n",
    "  %pip install contractions\n",
    "  import contractions\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Vectorization\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.sparse import csr_array, lil_array, save_npz, load_npz\n",
    "\n",
    "#Transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NN\n",
    "from torch import optim, nn\n",
    "\n",
    "#Tesauros\n",
    "from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y6Hamcl-S6e"
   },
   "source": [
    "# 2. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnnj2_Y6-VYO"
   },
   "outputs": [],
   "source": [
    "path_to_folder = '/data'\n",
    "\n",
    "#Import del excel\n",
    "excel_file_path = path_to_folder + '/projects.xlsx'\n",
    "df_projects = pd.read_excel(excel_file_path)\n",
    "\n",
    "#Ejecutar solo una vez para crear archivo parquet\n",
    "parquet_file_path = path_to_folder + '/projects.parquet'\n",
    "df_projects.to_parquet(parquet_file_path, engine='pyarrow')\n",
    "\n",
    "#Podemos cargar el archivo .parquet en vez del excel una vez esté creado para mayor rapidez\n",
    "# parquet_file_path = path_to_folder + '/projects.parquet'\n",
    "# df_projects = pd.read_parquet(parquet_file_path, engine='pyarrow')\n",
    "\n",
    "#Creamos un nuevo DataFrame que utilizaremos posteriormente\n",
    "df = pd.DataFrame()\n",
    "df['raw_text'] = df_projects.title + ' ' + df_projects.summary #concat de titulo y summary\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZfsOFTg-rzC"
   },
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWJO10fz-vne"
   },
   "outputs": [],
   "source": [
    "#Función que realiza todo el proceso de preprocesamiento\n",
    "def prepare_data(text):\n",
    "\n",
    "  preprocessed_text = BeautifulSoup(text,\"lxml\").get_text() #quitar etiquetas html\n",
    "  preprocessed_text = re.sub(r'https://\\S+|www\\.\\S+','',preprocessed_text) #quitar URLs\n",
    "  preprocessed_text = contractions.fix(preprocessed_text) #expandir contracciones\n",
    "  preprocessed_text = wordpunct_tokenize(preprocessed_text) #tokenizar\n",
    "  preprocessed_text = [preprocessed_text[i].lower() for i in range(len(preprocessed_text))] #minusculas\n",
    "  preprocessed_text = [preprocessed_text[i] for i in range(len(preprocessed_text)) if(preprocessed_text[i].isalnum())] #quitar caracteres especiales\n",
    "  wnl = WordNetLemmatizer() #lematizar\n",
    "  preprocessed_text = [wnl.lemmatize(el) for el in preprocessed_text]\n",
    "  stopwords_en = stopwords.words('english') #quitar stopwords\n",
    "  preprocessed_text = [preprocessed_text[i] for i in range(len(preprocessed_text)) if(preprocessed_text[i] not in stopwords_en)]\n",
    "\n",
    "  return preprocessed_text\n",
    "\n",
    "#Aplicamos el preprocesado y lo guardamos en una nueva columna\n",
    "df['lemmas'] = df['raw_text'].apply(prepare_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy_N_BXnFsQN"
   },
   "source": [
    "# 3.(Extra): Thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7sjYEfuFu7y"
   },
   "outputs": [],
   "source": [
    "# Si se desea aplicar esta tecnica, se deben ejecutar todas las celdas posteriores de nuevo\n",
    "# para la columna 'lemmas_thesaurus' del dataframe en lugar de la columna 'lemmas'\n",
    "\n",
    "# Función para obtener sinónimos de una palabra utilizando WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Función para expandir términos en un texto utilizando sinonimos, basada en preprocessing\n",
    "def expand_text_with_synonyms(text):\n",
    "    max_synonyms_per_token = 5\n",
    "    preprocessed_text = BeautifulSoup(text,\"lxml\").get_text() #quitar etiquetas html\n",
    "    preprocessed_text = re.sub(r'https://\\S+|www\\.\\S+','',preprocessed_text) #quitar URLs\n",
    "    preprocessed_text = contractions.fix(preprocessed_text) #expandir contracciones\n",
    "    preprocessed_text = wordpunct_tokenize(preprocessed_text) #tokenizar\n",
    "    preprocessed_text = [preprocessed_text[i].lower() for i in range(len(preprocessed_text))] #minusculas\n",
    "    preprocessed_text = [preprocessed_text[i] for i in range(len(preprocessed_text)) if(preprocessed_text[i].isalnum())] #quitar caracteres especiales\n",
    "    wnl = WordNetLemmatizer() #lematizar\n",
    "    preprocessed_text = [wnl.lemmatize(el) for el in preprocessed_text]\n",
    "    stopwords_en = stopwords.words('english') #quitar stopwords\n",
    "    preprocessed_text = [preprocessed_text[i] for i in range(len(preprocessed_text)) if(preprocessed_text[i] not in stopwords_en)]\n",
    "\n",
    "    expanded_tokens = []\n",
    "    for token in preprocessed_text:\n",
    "      # Obtener sinónimos y agregar hasta max_synonyms_per_token por token\n",
    "      synonyms = get_synonyms(token)\n",
    "      expanded_tokens.extend(synonyms[:max_synonyms_per_token])\n",
    "    return expanded_tokens\n",
    "\n",
    "\n",
    "# Se crea una nueva columna con los nuevos lemmas\n",
    "df['lemmas_thesaurus'] = df.raw_text.apply(expand_text_with_synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdxUlK1T_Hf1"
   },
   "source": [
    "# 4.Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scjsAYT-_xEo"
   },
   "source": [
    "## 4.1. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPI8RCIf_LIX"
   },
   "outputs": [],
   "source": [
    "#Crear un corpus iterable (eficiente en terminos de memoria) para crear un diccionario\n",
    "class IterableCorpus_fromdf:\n",
    "    def __init__(self, df):\n",
    "        self.__df = df\n",
    "    def __iter__(self):\n",
    "        for index, value in self.__df.lemmas.items():\n",
    "            yield value\n",
    "\n",
    "MyIterCorpus = IterableCorpus_fromdf(df)\n",
    "D = Dictionary(MyIterCorpus)\n",
    "no_below = 4 # Minimo número de documentos para guardar una palabra en el diccionario\n",
    "no_above = .80 # Maximo porcentaje de documentos en el que aparece una palabra para guardarla en el diccionario\n",
    "D.filter_extremes(no_below=no_below,no_above=no_above)\n",
    "\n",
    "#Obtenemos la representación TF-IDF\n",
    "bow = [D.doc2bow(doc) for doc in MyIterCorpus]\n",
    "tfidf = TfidfModel(bow)\n",
    "reviews_tfidf = tfidf[bow]\n",
    "df['emb_TFIDF'] = reviews_tfidf\n",
    "\n",
    "#Conversion a matriz sparse para su utilizacion en la posterior regresion\n",
    "n_tokens = len(D)\n",
    "num_docs = len(bow)\n",
    "corpus_tfidf_sparse = corpus2csc(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT8NY4NxAnjD"
   },
   "source": [
    "## 4.2. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxs6sJScApl7"
   },
   "outputs": [],
   "source": [
    "#Obtencion de la representacion word2vec\n",
    "\n",
    "def lemmas_to_line(row):\n",
    "    return ' '.join(row['lemmas'])\n",
    "\n",
    "iterable_sentences = [lemmas_to_line(row) for index, row in df.iterrows()]\n",
    "tokenized_sentences = [sentence.split() for sentence in iterable_sentences]\n",
    "vector_size = 200\n",
    "window = 5\n",
    "min_count = 10\n",
    "seed = 42\n",
    "sg = 1\n",
    "\n",
    "model_w2v = Word2Vec(sentences=tokenized_sentences,\n",
    "                 vector_size = vector_size,\n",
    "                 window = window,\n",
    "                 min_count = min_count,\n",
    "                 sg = sg,\n",
    "                 seed = seed,\n",
    "                 workers=4)\n",
    "wv = model_w2v.wv\n",
    "\n",
    "#Convertir en matriz sparse como promedio de los embeddings de las palabras que forman cada texto\n",
    "def get_review_vector(model, review):\n",
    "    tokens_without_OOV = [token for token in review if token in model]\n",
    "\n",
    "    if not tokens_without_OOV:\n",
    "        vec = csr_array(np.zeros(model.vector_size))\n",
    "        return vec\n",
    "    #Calculo del vector medio\n",
    "    review_vector = np.mean([model[token] for token in tokens_without_OOV], axis=0)\n",
    "    vec = csr_array(review_vector)\n",
    "    return vec\n",
    "\n",
    "# Guardado en matriz sparse\n",
    "embedding_size = wv.vector_size\n",
    "corpus_word2vec_sparse = lil_array((len(df), embedding_size), dtype=np.float32)\n",
    "\n",
    "for i,row in zip(range(len(df)),df.iterrows()):\n",
    "    row = row[1]\n",
    "    review_tokens = row['lemmas']\n",
    "    review_vector_sparse = get_review_vector(wv, review_tokens)\n",
    "    corpus_word2vec_sparse[i, :] = review_vector_sparse\n",
    "\n",
    "corpus_word2vec_sparse = csr_array(corpus_word2vec_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy298iIDCM23"
   },
   "source": [
    "## 4.3. Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lqxFhHyCPD9"
   },
   "outputs": [],
   "source": [
    "# Seleccion de modelo\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "#Se selecciona la GPU si esta disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "#Lista para guardar los embeddings\n",
    "review_embeddings_roberta = []\n",
    "\n",
    "batch_size = 8\n",
    "for i in tqdm(range(0, len(df['raw_text']), batch_size)):\n",
    "    batch_reviews = df['raw_text'][i:i+batch_size]\n",
    "    #Tokenizacion\n",
    "    tokens = tokenizer(batch_reviews.tolist(), return_tensors='pt', truncation=True, padding=True)\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "\n",
    "    cls_embedding = output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    review_embeddings_roberta.append(cls_embedding)\n",
    "\n",
    "\n",
    "# Conversion a matriz de numpy\n",
    "review_embeddings_roberta = np.vstack(review_embeddings_roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI8uRdx4C1Rc"
   },
   "source": [
    "# 5. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEdG9BLMDbxd"
   },
   "source": [
    "## 5.1. Train/test sets definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImUW8vLOC593"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "\n",
    "#Obtener el numero de publicaciones y patentes\n",
    "df_projects.patentID.fillna(value='[]', inplace=True)\n",
    "df_projects.publicationID.fillna(value='[]', inplace=True)\n",
    "\n",
    "#Fila index=6623 error tipografico en publicationID (falta '])\n",
    "for index,row in df_projects.iterrows():\n",
    "  if(str(row.publicationID)[-1]!=']'):\n",
    "    df_projects.publicationID[index] = df_projects.publicationID[index]+'\\']'\n",
    "y = [len(eval(df_projects.publicationID[i])) + len(eval(df_projects.patentID[i])) for i in range(df_projects.shape[0])]\n",
    "\n",
    "#Elegimos cada vez una fila diferente para elegir el modelo deseado en cada caso. Dejamos por defecto word2vec aunque puedes elegir cualquiera\n",
    "\n",
    "# embeddings = corpus_tfidf_sparse.toarray()\n",
    "embeddings = corpus_word2vec_sparse.toarray()\n",
    "# embeddings = embeddings_roberta\n",
    "# embeddings = np.asarray(corpus_word2vec_dense)\n",
    "# embeddings = corpus_tfidf_sparse_thesaurus.toarray()\n",
    "# embeddings = corpus_word2vec_sparse_thesaurus.toarray()\n",
    "\n",
    "# Crear un MultiLabelBinarizer para convertir listas de países en vectores binarios\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels_binary = mlb.fit_transform(df_projects['coordinatorCountry'])\n",
    "numeros_decimales = np.dot(labels_binary, 2 ** np.arange(labels_binary.shape[1])[::-1])\n",
    "matriz_columna = numeros_decimales.reshape(-1, 1)\n",
    "df_projects['coordinatorCountry_bin'] = matriz_columna\n",
    "\n",
    "#Normalización de los datos\n",
    "scaler = StandardScaler()\n",
    "y = np.array(y)[:,np.newaxis]\n",
    "\n",
    "#Si queremos introducir más columnas del excel parar mayor información del conjunto, descomentamos las posteriores líneas\n",
    "data = np.hstack((embeddings,\n",
    "                  # np.array(df_projects.totalCost)[:,np.newaxis],\n",
    "                  # np.array(df_projects.ecMaxContribution)[:,np.newaxis],\n",
    "                  # np.array(df_projects.rcn)[:,np.newaxis],\n",
    "                  # np.array(df_projects.coordinatorCountry_bin)[:,np.newaxis],\n",
    "                  y))\n",
    "scaler.fit(data)\n",
    "data_scaled = scaler.transform(data)\n",
    "\n",
    "#Crear los subconjuntos de prueba y test\n",
    "X = data_scaled[:,:-1]\n",
    "y = data_scaled[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def sparse_matrix2sparse_tensor(sm):\n",
    "  coo = sm.tocoo()\n",
    "  indices = torch.LongTensor([coo.row, coo.col])\n",
    "  values = torch.FloatTensor(coo.data)\n",
    "  shape = torch.Size(sm.shape)\n",
    "  st = torch.sparse.FloatTensor(indices, values, shape)\n",
    "  return st\n",
    "\n",
    "\n",
    "# Convert to Torch tensors\n",
    "# X_train_torch = sparse_matrix2sparse_tensor(X_train)\n",
    "# X_test_torch = sparse_matrix2sparse_tensor(X_test)\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "input_dim=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_COpbbfDg5Q"
   },
   "source": [
    "## 5.2. RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-0sm2jkDkL6"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Cogemos sólo los 5000 primeros textos por rapidez de ejecución, si se quiere se pueden utilizar más textos de los disponibles\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:5000,:], y[:5000], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Crea el modelo de regresión con RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators=120, max_features='sqrt', random_state=42, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrena el modelo\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Método de validación cruzada\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Evalúa el rendimiento del modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Error cuadrático medio: {mse}\")\n",
    "print(f'Puntuación validación cruzada: {cv_scores}')\n",
    "print(f'Media validación cruzada: {cv_scores.mean()}')\n",
    "print(f'Desviación estándar: {cv_scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLQxc2s4Dkri"
   },
   "source": [
    "## 5.3. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzUVURFSDnSH"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#Modelo de la red, ciertas capas de dropout estan comentadas ya que se han realizado varias pruebas con ellas\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 240),\n",
    "    nn.BatchNorm1d(240), #Utilizar esta capa solo en los casos especificados en la memoria\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(240, 120),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.5),\n",
    "    nn.Linear(120, 60),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.8),\n",
    "    nn.Linear(60, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(30, 15),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(15, 7),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(7, 1)\n",
    ")\n",
    "\n",
    "#Seleccion de funcion de perdidas y optimizador\n",
    "loss=nn.MSELoss()\n",
    "optimizers=optim.Adam(params=model.parameters(), lr = 0.01, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "#Verificar si la GPU está disponible y enviar variables\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo utilizado:\", device)\n",
    "\n",
    "X_train_torch = X_train_torch.to(device)\n",
    "y_train_torch = y_train_torch.to(device)\n",
    "X_test_torch = X_test_torch.to(device)\n",
    "y_test_torch = y_test_torch.to(device)\n",
    "model = model.to(device)\n",
    "loss = loss.to(device)\n",
    "\n",
    "#Entrenamiento\n",
    "num_of_epochs=500\n",
    "loss_train = np.zeros(num_of_epochs)\n",
    "loss_test = np.zeros(num_of_epochs)\n",
    "\n",
    "#Parametros para early stopping\n",
    "best_test_loss = float('inf')\n",
    "patience = 150\n",
    "early_stop_counter = 0\n",
    "epoch_early_stop = num_of_epochs\n",
    "best_epoch = 0\n",
    "best_weights = None\n",
    "\n",
    "for epoch in range(num_of_epochs):\n",
    "  #Entrenamiento para epoch actual\n",
    "  model.train()\n",
    "  y_train_prediction=model(X_train_torch)\n",
    "  loss_value=loss(y_train_prediction.squeeze(),y_train_torch.squeeze())\n",
    "  loss_train[epoch] = loss_value\n",
    "\n",
    "  #Retropropagacion y actualizacion de pesos\n",
    "  optimizers.zero_grad()\n",
    "  loss_value.backward()\n",
    "  optimizers.step()\n",
    "\n",
    "  #Evaluacion de test para la epoch actual\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    y_test_prediction=model(X_test_torch)\n",
    "    loss_test[epoch] = loss(y_test_prediction.squeeze(),y_test_torch.squeeze())\n",
    "\n",
    "  #Comprobacion del early stop\n",
    "  if (loss_test[epoch] < best_test_loss):\n",
    "    best_test_loss = loss_test[epoch]\n",
    "    best_epoch = epoch\n",
    "    early_stop_counter = 0\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "  else:\n",
    "    early_stop_counter += 1\n",
    "\n",
    "  if (early_stop_counter >= patience):\n",
    "    print(f'Early stopping at epoch {epoch}')\n",
    "    epoch_early_stop = epoch\n",
    "    break\n",
    "\n",
    "  #Evolucion del error con las epochs\n",
    "  if epoch % 20 == 0:\n",
    "    print(f'[epoch:{epoch}]: Training loss={loss_train[epoch]}, Test loss={loss_test[epoch]}')\n",
    "\n",
    "\n",
    "#Evaluacion final\n",
    "with torch.no_grad():\n",
    "  #Se recupera el mejor modelo\n",
    "  model.load_state_dict(best_weights)\n",
    "  model.eval()\n",
    "  y_test_prediction=model(X_test_torch)\n",
    "  test_loss=loss(y_test_prediction.squeeze(),y_test_torch)\n",
    "  print(f'Best test loss value : {best_test_loss:.4f} obtainted at epoch {best_epoch}')\n",
    "\n",
    "#Si se desea visualizar una grafica con la evolucion de los errores de train y test\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train[0:epoch_early_stop+1], 'b'), plt.plot(loss_test[0:epoch_early_stop+1], 'r'), plt.legend(['train', 'test']), plt.title('MSE loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
